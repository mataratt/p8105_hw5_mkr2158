---
title: "Homework 5"
author: Matariya Rattanapan (mkr2158)
output: github_document
---

Loading in key packages and setting design elements.

```{r setup, message=FALSE}
library(tidyverse)

set.seed(1)
n        = 30
sigma    = 5
mu_value = 0:6

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

```

## Question 1

N/A

## Question 2

Creating the one-sample t-test function.
```{r}
one_ttest_fx = function(mu) {
  
      x = rnorm(n, mean = mu, sd = sigma)
      
      broom::tidy(t.test(x, mu = 0)) |> 
      dplyr::select(estimate, p.value)

}
```

Generate 5,000 datasets using function. Simulated this for mu = 0:6. Rejection of null hypothesis if p-value > α = 0.05.

```{r}
results_df =
  expand_grid(
    mu = mu_value, 
    iter = 1:5000) |>
  mutate(
    test_results = map(mu, one_ttest_fx)
    ) |>
  unnest(test_results) |>
  mutate(
    reject = p.value < 0.05
    )
```

Total number of observations in `results_df` is 35,000. 

Making a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. 

```{r}
power_df =
  results_df |>
  group_by(mu) |>
  summarize(power = mean(reject))

power_df |> 
  ggplot(aes(x = mu, y = power)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  labs(
    title = "Power of One-Sample T-Test",
    x = expression(True~mu),
    y = "Power"
  )
```

* From the graph, we see that as the true μ (mean) is closer to 0, the power is closest to a = 0.05 (level of significance 5%).
* As the true μ (mean) increases, power, or the proportion of rejecting the null increases as well. Thus, a large true μ translates to a larger effect size which means the one-sample t-test has greater power, reflecting a positive association between effect size and power. 

Making a plot showing the average estimate of μ on the y axis and the true value of μ the x axis.
The plot also contains an overlay of the average estimate of μ only in samples for which the null was rejected.

```{r}
avg_mu =
  results_df |>
  group_by(mu) |>
  summarize(
    mu_hat = mean(estimate),
    mu_hat_rejected = mean(estimate[reject])
  ) |>
  pivot_longer(
    cols = c(mu_hat, mu_hat_rejected),
    names_to = "mu_hat",
    values_to = "average"
  )

avg_mu |> 
  ggplot(aes(x = mu, y = average, color = mu_hat)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  labs(
    title = "Average Estimated μ and Rejected μ y True μ",
    x = expression(True~mu),
    y = expression(Average~hat(mu)),
    color = NULL
  ) +
  scale_color_discrete(
    labels = c("All Samples", "Only When Null Rejected")
  )
```

* The red line represents the average estimated mean of all samples, whereas the blue lines representthe average estimate of mean in samples only when the null is rejected. When the true μ is small, the average mean for samples only when null reject tends to be higher than that of the overall sample mean. Generally however, the sample average of μ across the test for which the null is rejected tends to overestimate (be higher) than the true value of μ since we are looking at only sample for which the null is rejected. 


## Problem 3

Reading in the raw `homicide` dataset.
```{r, message = FALSE}
homicide = 
  read_csv("data/homicide-data.csv")
```

* The raw data contains `r nrow(homicide)` observations and `r ncol(homicide)` variables. 
* The variables for this raw dataset include: `uid`, `reported_date`, `victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat`, `lon`, and `disposition`.


Creating a city_state variable and summarizing within cities the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

I provided an example table showing the first 10 cities in alphabetic order with this summary.

```{r}
homicide_summary =
homicide |> 
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) |> 
  group_by(city_state) |>
  summarise(
    total_homicides   = n(),
    unsolved_homicides = sum(unsolved)
  )

  knitr::kable(head(homicide_summary,10))
```

Estimating the proportion and confidence intervals of homicides that are unsolved in Baltimore, MD. 

```{r}
balt_md = 
homicide_summary |> 
  filter(
    city_state == "Baltimore, MD"
  ) |> 
  mutate(
    proportion = list(broom::tidy
                      (prop.test(x = unsolved_homicides, n = total_homicides))
  )
  ) |> 
  unnest(proportion) |> 
  select(estimate, conf.low, conf.high)
```

The estimated proportion of unsolved homicides in Baltimore, MD is `r round(balt_md$estimate, 3)`, with a 95% confidence interval from `r round(balt_md$conf.low, 3)` to `r round(balt_md$conf.high, 3)`

Running prop.test for each of the cities in the `homicide` dataset and extracting the proportion of unsolved homicides and the confidence interval for each.

I have provided an example with the first 10 cities in alphabetical order with this summary.

```{r}
city_proportions =
  homicide_summary |> 
  mutate(
    proportion = map2(
      unsolved_homicides,
      total_homicides,
      ~ broom::tidy(prop.test(.x, .y))
    )
  ) |> 
  unnest(proportion) |> 
  select(city_state, estimate, conf.low, conf.high)

knitr::kable(head(city_proportions, 10))
```

Creating a plot that shows the estimates and confidence intervals for each city.

```{r, fig.width=10, fig.height=8}
city_proportions |>
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbar(
    aes(xmin = conf.low, xmax = conf.high)
  ) +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "Proportion of Unsolved Homicides",
    y = "City"
  )
```




